---
layout:     post
title:      支持向量机
subtitle:   Support Vector Machine
date:       2020-03-10
author:     朱晓旭
header-img: img/bg.jpg
catalog: true
tags:
    - 支持向量机
    - SVM
    - Support Vector Machine
    - 最优化
    - 核方法
    - kernel method
    - 对偶	
    - 拉格朗日乘子法	
    - KKT条件	
---


# 支持向量机

# 简介
支持向量机(support vector machine)是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。
以上是维基百科的对SVM的定义。      
用通俗的话讲，它是一种统计机器学习的分类方法，根据训练集分布去寻找一个最大间隔超平面。1963年被提出，92年通过核方法可以实现非线性分类。			
在支持向量机被普遍接受后，核技巧(kernel trick) 被人们用到了机器学习的几乎每一个角落，核方法也逐渐成为 机器学习的基本内容之一。
支持向量机在分类问题中有着重要地位，其主要思想是最大化两类之间的间隔。按照数据集的特点我们采用的不同算法：

1.  对于线性可分问题，采用hard-margin SVM。
2.  对于允许少部分错误点的线性可分问题，采用soft-margin SVM。
3.  对于非线性问题，完全不可分，采用kernel method。

# hard-margin SVM
### margin and sv
给定训练样本data:
<center>$$\{(x_{i},y_{i})\}_{i=1}^{N}$$</center>
<center>$$y_{i}\in\{1,-1\},x_{i}\in{\mathbb{R}}^{d}$$</center>
分类器的预测目标是：给定x情况下，预测y属于哪类。
分类学习最基本的想法就是基于训练集data在样本空间中找到一个划分超平面，将不同类别的样本分开。
在样本空间中，划分超平面可以通过下面的线性方程描述：
<center>$$w^{T}x_{i}+b=0$$</center>
$w=(w_{1};w_{2};...;w_{d})$是法向量，决定了超平面的方向；b是位移项，决定了超平面与原点之间的距离。划分超平面可被法向量和位移项唯一的确定。
![](svm_2.png)
实现分类超平面的约束条件则是(其中1代表正类，-1代表反类)：   
<center>$$w^{T}x_{i}+b>0,y_{i}=1$$</center>
<center>$$w^{T}x_{i}+b<0,y_{i}=-1$$</center>
即：  
<center>$$y_{i}(w^{T}x_{i}+b)>0,i \in \{1,2,3...,N\} $$</center>
而从下图可以看到，能把训练样本分开的划分超平面可能有很多。  
![](/img/svm_1.jpg)
我们应该努力寻找的应该是距离两者最中间的那条实线，因为它对噪声的容忍最好，也就是说它的鲁棒性更好，对集外的样本泛化能力更强。距离样本点最近的点与超平面的距离是所有平面中最大的。它就是最大间隔超平面：        
在超平面$w^{T}x_{i}+b=0$确定的情况下，$|w^{T}x_{i}|$能够相对的表示采样点距离超平面的远近，而通过判断$w^{T}x_{i}+b$的符号是否与y_{i}一致可以判断分类是否正确。所以可以用$y_{i}(w^{T}x_{i}+b)$代表分类的正确性与确信度。这就是函数间隔的定义：
<center>$$fucntional margin = y_{i}(w^{T}x_{i}+b)$$</center>
这样定义存在一个问题，同比例的改变法向量w和位移项的值(比如2w，2b)，超平面没有改变，间隔却变成原来的两倍。因此我们可以对划分超平面的法向量加以约束(如规范化，${\left\|w\right\|_2}=1$),这时函数间隔变成几何间隔(geometric margin)。我们以此定义距离的远近。
<center>$$geometric margin = \frac{y_{i}(w^{T}x_{i}+b)}{\left\|w\right\|_2}$$</center>
同样的，几何间隔能够代表分类的正确性与确信度。正确性体现在符号，确信度体现在distance。x为二维时，实际上就是点到直线的距离；x为高维时，指的是采样点到超平面的距离。   
我们所指的margin是支持向量的margin，其中支持向量是geometric margin最小的采样点。
### 约束优化问题
由此，分类问题转化为了约束优化问题:     
<center>$$ \max_{w,b}\min_{x_{i},i in 1,2,3..N}\frac{1}{\left\|w\right\|_2}y_{i}(w^{T}x_{i}+b)=\max_{w,b}\frac{1}{\left\|w\right\|_2}\min_{x_{i},i \in \{1,2,3..N\}y_{i}(w^{T}x_{i}+b)$$</center>
<center>$$s.t. y_{i}(w^{T}x_{i}+b)>0$$</center>
这样的话：    
<center>$$\max_{w,b}\frac{1}{\left\|w\right\|_2}$$</center>
<center>$$s.t. \min y_{i}(w^{T}x_{i}+b)=1 $$</center>
> [统计学习方法](https://github.com/zhuxiaoxuhit/DeepLearning/blob/master/books/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.pdf)解释：函数间隔r的取值并不影响最优化问题的解。事实上，假设将w和b按照比例改编为nw和nb，这时函数间隔变为nr。函数间隔的这一改变对上面问题的不等式约束没有影响，对目标函数优化也没有影响，也就是说改变r，产生的是一个等价的最优化问题。      
理解：   
<center>$$\exists{ r>0},s.t. \min_{x_{i},y_{i},i \in \{1,2,3,4...N\}y_{i}(w^{T}x_{i}+b)=r $$</center>
而r的取值可以是任意大于零的数，为了方便计算，此处取为1。   

我们在求解最优化问题时，往往采用最小化：      
<center>$$\min_{w,b}\frac{1}{2}w^{T}w$$</center>
<center>$$s.t.  \min y_{i}(w^{T}x_{i}+b)=1 $$</center>
这是注意到问题本身是一个凸二次规划(convex quadratic programming)问题，能直接用现成的优化计算包求解，但我们可以有更高效的办法。
### 拉格朗日乘子法与对偶技巧
应用[拉格朗日乘子法](https://zh.wikipedia.org/zh-cn/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)
<center>$$ L(w,b,a)=\frac{1}{2}w^{T}w+\sum_{i=1}^{N}a_{i}(1-y_{i}(w^{T}x_{i}+b))$$</center>
其中，a是N维的向量。
因此：   
<center>$$\min_{w,b}\max_{a}L(w,b,a)$$</center>
<center>$$s.t. a>=0 $$</center>
解释为什么a>=0:    
如果$(1-y_{i}(w^{T}x_{i}+b))>0$,a可以是大于0的任意一个数，那$\max_{a}L(w,b,a)$本身是没有意义的。     
如果$(1-y_{i}(w^{T}x_{i}+b))<=0$,a取0，才和原式恒等。    
为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性通过求解对偶问题(dual problem)得到原始问题(primal problem)的最优解。
<center>$$\max_{a}\min_{w,b}L(w,b,a)$$</center>
<center>$$s.t. a_{i}>=0 $$</center>
为什么把原问题转换为对偶问题：一是对偶问题更容易求解(抛弃约束给a，因此可以可以通过求偏导求解$\min_{w,b}L(w,b,a)$,此使把a看作常量),二是自然引入核函数，进而推广到非线性问题。			       
为什么是强对偶关系：由[统计学习方法-李航](https://github.com/zhuxiaoxuhit/DeepLearning/blob/master/books/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.pdf)第225页的附录C拉格朗日对偶性中定理C.3可知该凸优化问题天然满足KKT对偶互补条件，因此为强对偶。        
KKT条件(原问题和对偶问题具有强对偶关系的充要条件)：
<center>$$\frac{\partial{L}}{\partial{w}}=0$$</center>
<center>$$\frac{\partial{L}}{\partial{b}}=0$$</center>
<center>$$\frac{\partial{L}}{\partial{a}}=0$$</center>
<center>$$a_{i}>=0$$</center>
<center>$$1-y_{i}(w^{T}x_{i}+b)<=0$$</center>
<center>$$a_{i}(1-y_{i}(w^{T}x_{i}+b))=0$$</center>

### 问题求解
为了得到对偶问题的解，先求L(w,b,a)对w，b的极小，再求对a的极大。
对w,b求偏导得:
<center>$$\sum_{i=1}^{N}a_{i}y_{i}=0$$</center>
<center>$$w=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$</center>
把结果带入对偶式，得：
<center>$$\max_{a}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}x_{i}^{T}x_{j}+\sum_{i=1}^{N}a_{i}$$</center>
<center>$$s.t. a_{i}>=0 , \sum_{i=1}^{N}a_{i}y_{i}=0 $$</center>  
然后转为最小化问题：
<center>$$\min_{a}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}x_{i}^{T}x_{j}-\sum_{i=1}^{N}a_{i}$$</center>
<center>$$s.t. a_{i}>=0 , \sum_{i=1}^{N}a_{i}y_{i}=0 $$</center>  
可以求解a。
其中$w^{*}$在对L求导时已经求出：
<center>$$w^{*}=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$</center>
由KKT条件的后三个，只要样本点不是支持向量，a_{i}必须为0。    
对于支持向量$x_{s},y_{s}$,其$1-y_{s}({(w^{*})}^{T}x_{s}+b^{*})=0$    
由此可得：
<center>$$b^{*}=y_{s}-\sum_{i=1}^{N}a_{i}y_{i}x_{i}^{T}x_{s}$$</center>
因此，原问题的解也为：
<center>$$w^{*}=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$</center>
<center>$$b^{*}=y_{s}-\sum_{i=1}^{N}a_{i}y_{i}x_{i}^{T}x_{s}$$</center>

# soft-margin SVM
软间隔同样是针对线性可分问题。它的核心思想是：允许少许错误。
<center>$$\min_{w,b}\frac{1}{2}w^{T}w + loss$$</center>
loss可以是分错的数量，使用0/1损失函数。当数据存在噪声的时候用软间隔。

# kernel Method
### 维度转换
我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。而对于数据本身线性不可分的情况，无法用线性模型将正负例分开。   		
然而低维线性不可分的输入空间进行高维映射，往往在高维更易线性可分。可以看下面的例子：			
![](/img/svm_3.png)
二维平面上的点无法用一条直线分开，可以将其按照一定规则映射到三维空间中，可以用超平面将其分开。
![](/img/svm_4.gif)	

维度转换： 
设原空间为$X\subset{\mathbb{R}}^{d},x_{i}\in{X}$,新空间为$Z\subset{\mathbb{R}}^{d},z_{i}\in{Z}$
<center>$$z_{i} = \phi(x_{i})$$</center>
### 带来问题
维度转换后在hard-margin svm中求解对偶式，得：
<center>$$\max_{a}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}x_{i}^{T}x_{j}-\sum_{i=1}^{N}a_{i}$$</center>
<center>$$s.t. a_{i}>=0 , \sum_{i=1}^{N}a_{i}y_{i}=0 $$</center>  

由此带来的问题时对偶问题带来的内积$x_{i},x_[j}^{T}$：
<center>$$<f(x_{i}),f(x_{i})^{T}>$$</center>
### 核方法，核技巧与核函数
核方法思想：通俗的发讲，就是寻找高维维度转换f(x),使我们能够轻松计算内积。核函数就是他们的内积表示。       
我们使用的时候一般通过差表选择合适的核函数。   
![](/img/svm_3.jpg)


### 参考

- [机器学习-周志华(西瓜书)](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)
- [统计学习方法第二版-李航](https://github.com/zhuxiaoxuhit/DeepLearning/blob/master/books/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.pdf)




