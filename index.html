---
layout: default
title: "Home"
description: "Speech AI researcher focusing on large speech models and speech generation"
---

<!-- About Me Section -->
<div class="container" style="margin-top: 50px;">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="site-heading">
                <h1 style="margin-bottom: 30px;"><i class="fa fa-home"></i> About Me</h1>
            </div>
            
            <!-- Replace the About Me block using more compact layout -->
            <div style="margin-bottom: 20px;">
                <div class="row align-items-center" style="gap: 20px;">
                    <div class="col-md-3 text-center">
                        <img src="/img/zhuxiaoxu.png" alt="Xiaoxu Zhu" class="img-responsive" style="width: 100%; max-width: 160px; border-radius: 10px;">
                    </div>
                    <div class="col-md-9">
                        <p style="font-size: 16px; line-height: 1.6; margin: 0 0 8px 0;">
                            I am Xiaoxu Zhu (朱晓旭), a Speech AI researcher focusing on <strong>large speech models</strong> and <strong>speech generation</strong>. I have been working on speech algorithm research and development at <strong>SenseTime</strong> since 2021. I have also worked or interned at <strong>Siemens</strong>, <strong>Cheetah Mobile (OrionStar)</strong>, and <strong>Shanghai AI Laboratory</strong>.
                        </p>
                        <p style="font-size: 16px; line-height: 1.6; margin: 0;">
                            I received my Bachelor's degree in Materials Forming and Control Engineering from <strong>Harbin Institute of Technology</strong> in 2016, and my Master's degree in Informatics and Computing Technology from <strong>Peter the Great St. Petersburg Polytechnic University</strong> in 2019. I am currently pursuing my Master's degree in Information Management at <strong>Tsinghua University</strong>, expected to graduate in 2025.
                        </p>
                    </div>
                </div>
            </div>
            
            <!-- Publications Section -->
            <div id="publications" style="margin-top: 30px; padding-top: 80px;">
                <h2 style="color: #333; margin-bottom: 20px;"><i class="fa fa-book"></i> Publications</h2>
                
                <!-- Publication 1 -->
                <div class="publication-item" style="margin-bottom: 25px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_1.png" alt="Paper 1" class="img-responsive" style="width: 100%; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability</h4>
                            <p style="color: #666; margin: 5px 0;">
                                <strong>Xiaoxu Zhu</strong>, Junhua Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Technical Report</em>, 2025
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://arxiv.org/pdf/2507.17851" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract1')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex1')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    Speech pretrained models contain task-specific information across different layers, but decoupling content and timbre information remains challenging as removing speaker-specific information often causes content loss. Current research lacks direct metrics to quantify timbre residual in model encodings, relying on indirect evaluation through downstream tasks. This paper addresses these challenges through interpretability-based speaker disentanglement in speech pretraining models. We quantitatively evaluate timbre residual in model embeddings and improve speaker disentanglement using interpretive representations. Our contributions include: (1) InterpTRQE-SptME Benchmark - a timbre residual recognition framework using interpretability. The benchmark concatenates content embeddings with timbre embeddings for speaker classification, then applies Gradient SHAP Explainer to quantify timbre residual. We evaluate seven speech pretraining model variations. (2) InterpTF-SptME method - an interpretability-based timbre filtering approach using SHAP Noise and SHAP Cropping techniques. This model-agnostic method transforms intermediate encodings to remove timbre while preserving content. Experiments on VCTK dataset with HuBERT LARGE demonstrate successful content preservation and significant speaker disentanglement optimization. Results show the SHAP Noise method can reduce timbre residual from 18.05% to near 0% while maintaining content integrity, contributing to enhanced performance in content-related speech processing tasks and preventing timbre privacy leakage.
                                </p>
                            </div>
                            <div id="bibtex1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@article{zhu2024speaker,
  title={Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability},
  author={Zhu, Xiaoxu and Li, Junhua},
  journal={arXiv preprint arXiv:2507.17851},
  year={2024}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 2 -->
                <div class="publication-item" style="margin-bottom: 25px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_2.png" alt="Paper 2" class="img-responsive" style="width: 100%; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese</h4>
                            <p style="color: #666; margin: 5px 0;">
                                Song Zhang, Ken Zheng, <strong>Xiaoxu Zhu</strong>, Baoxiang Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Interspeech</em>, 2022
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://www.isca-archive.org/interspeech_2022/zhang22b_interspeech.pdf" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract2')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex2')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. Firstly, we create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained Chinese BERT with 741 new Chinese monophonic characters and adding a corresponding embedding layer for new tokens, which is initialized by the embeddings of source Chinese polyphonic characters. In this way, we can turn the polyphone disambiguation task into a pre-training task of the Chinese polyphone BERT. Experimental results demonstrate the effectiveness of the proposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%) improvement of average accuracy compared with the BERT-based classifier model, which is the prior state-of-the-art in polyphone disambiguation.
                                </p>
                            </div>
                            <div id="bibtex2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{zhang2022polyphone,
  title={A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese},
  author={Zhang, Song and Zheng, Ken and Zhu, Xiaoxu and Li, Baoxiang},
  booktitle={Proc. Interspeech 2022},
  year={2022},
  note={Accepted for INTERSPEECH 2022}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 3 -->
                <div class="publication-item" style="margin-bottom: 25px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_3.png" alt="Paper 3" class="img-responsive" style="width: 100%; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy</h4>
                            <p style="color: #666; margin: 5px 0;">
                                C. Ding, D. Zong, B. Li, S. Zhang, <strong>X. Zhu</strong>, G. Zhong, D. Zhou
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>IEEE/ACM MuSe-Mimic</em>, 2023
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://dl.acm.org/doi/10.1145/3606039.3613113" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract3')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex3')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    In this paper, we present the solution to the MuSe-Mimic subchallenge of the 4th Multimodal Sentiment Analysis Challenge. This sub-challenge aims to predict the level of approval, disappointment and uncertainty in user-generated video clips. In our experiments, we found that naive joint training of multiple modalities by late fusion would result in insufficient learning of unimodal features. Moreover, different modalities contribute differently to MuSe-Mimic. Relying solely on multimodal features or treating unimodal features equally may limit the model's generalization performance. To address these challenges, we propose an efficient multimodal transformer equipped with a modality-aware adaptive training strategy to facilitate optimal joint training on multimodal sequence inputs. This framework holds promise in leveraging cross-modal interactions while ensuring adequate learning of unimodal features. Our model achieves the mean Pearson's Correlation Coefficient of .729 (ranking 2nd), outperforming official baseline result of .473. Our code is available at https://github.com/dingchaoyue/Multimodal-Emotion-Recognition-MER-and-MuSe-2023-Challenges.
                                </p>
                            </div>
                            <div id="bibtex3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{ding2023multimodal,
  title={Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy},
  author={Ding, C. and Zong, D. and Li, B. and Zhang, S. and Zhu, X. and Zhong, G. and Zhou, D.},
  booktitle={Proceedings of the 4th International on Multimodal Sentiment Analysis Workshop and Challenge},
  pages={263--270},
  year={2023},
  organization={ACM}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Contributions Section -->
<div id="contributions" class="container" style="margin-top: 30px; padding-top: 80px;">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <h2 style="color: #333; margin-bottom: 20px;"><i class="fa fa-trophy"></i> Contributions</h2>
            
            <!-- Research Projects -->
            <div style="margin-bottom: 25px;">
                <h3 style="color: #0085a1; margin-bottom: 20px;"> Research Projects</h3>
                <ul style="list-style: none; padding: 0;">
                    <li style="margin-bottom: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #0085a1;">
                        <strong>基于生成式大模型的公路路基突发性灾害预警技术与方法</strong><br>
                        <span style="color: #666; font-size: 14px;"><b>专题负责人</b> | 国家自然科学基金委员会高技术研究发展中心 | SQ2024YFB2600035</span>
                    </li>
                    <li style="margin-bottom: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #0085a1;">
                        <strong>基于语义知识图谱的建筑工程标准国际化共性关键技术</strong><br>
                        <span style="color: #666; font-size: 14px;"><b>项目骨干</b> | 中国21世纪议程管理中心 | SQ2024YFC3800085</span>
                    </li>
                </ul>
            </div>

            <!-- Patents -->
            <div style="margin-bottom: 25px;">
                <h3 style="color: #0085a1; margin-bottom: 20px;"> Patents</h3>
                <ul style="list-style: none; padding: 0;">
                    <li style="margin-bottom: 10px; padding: 10px; background: #f8f9fa; border-left: 4px solid #5cb85c;">
                        <strong>多音字读音预测网络的训练方法、语音生成方法及装置</strong> [CN115273809A]
                    </li>
                    <li style="margin-bottom: 10px; padding: 10px; background: #f8f9fa; border-left: 4px solid #5cb85c;">
                        <strong>残差网络的训练和语音合成方法、装置、设备及介质</strong> [CN112562655A]
                    </li>
                    <li style="margin-bottom: 10px; padding: 10px; background: #f8f9fa; border-left: 4px solid #5cb85c;">
                        <strong>模型训练和语音合成方法、装置、设备及介质</strong> [CN116206591A]
                    </li>
                    <li style="margin-bottom: 10px; padding: 10px; background: #f8f9fa; border-left: 4px solid #5cb85c;">
                        <strong>一种模型训练和语音合成方法、装置、设备及介质</strong> [CN115294955A]
                    </li>
                </ul>
            </div>

            <!-- Competitions -->
            <div style="margin-bottom: 25px;">
                <h3 style="color: #0085a1; margin-bottom: 20px;"> Competitions & Awards</h3>
                <ul style="list-style: none; padding: 0;">
                    <li style="margin-bottom: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e;">
                        <strong>2024 Intel Mini Hackathon</strong> - Excellent Work Award<br>
                        <span style="color: #666; font-size: 14px;">Recognized for outstanding performance in Intel's AI/ML hackathon competition</span>
    </li>
                    <li style="margin-bottom: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e;">
                        <strong>2023 ACM MuSe-Mimic Subchallenge</strong> - Second Place<br>
                        <span style="color: #666; font-size: 14px;">Achieved 2nd place in the Multimodal Sentiment Analysis Challenge</span>
    </li>
</ul>
            </div>

            <!-- Open Source Contributions -->
            <div style="margin-bottom: 15px;">
                <h3 style="color: #0085a1; margin-bottom: 20px;"> Open Source Contributions</h3>
                <div style="padding: 20px; background: #f8f9fa; border-left: 4px solid #d9534f; border-radius: 5px;">
                    <h4 style="margin-top: 0; color: #333;">
                        <a href="https://github.com/xiph/LPCNet/commit/c1e85f88d908533c5600dbdd800ac589e15747f4" target="_blank" style="color: #0085a1; text-decoration: none;">
                            <i class="fa fa-github"></i> LPCNet - Pre-compute GRU B Conditioning
                        </a>
                    </h4>
                    <p style="color: #666; margin: 10px 0; font-size: 14px; line-height: 1.6;">
                        <strong>Performance Optimization:</strong> Implemented pre-computation of GRU B conditioning vectors to achieve approximately <strong>10% speed improvement</strong> in LPCNet inference. This optimization reduces computational overhead by caching frequently used conditioning vectors, significantly improving real-time speech synthesis performance.
                    </p>
                    <p style="color: #666; margin: 10px 0; font-size: 14px; line-height: 1.6;">
                        <strong>Technical Details:</strong> The contribution involves restructuring the GRU computation pipeline to separate feature-dependent conditioning from sample-dependent processing, enabling batch pre-computation and reducing redundant calculations during inference.
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Contact Section -->
<div id="contact" class="container" style="margin-top: 20px; padding-top: 80px;">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <h2 style="color: #333; margin-bottom: 30px;"><i class="fa fa-envelope"></i> Contact</h2>
            
            <div style="padding: 20px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #0085a1;">
                <p style="font-size: 16px; line-height: 1.6; margin-bottom: 15px;">
                    I am always eager to connect and exchange ideas with fellow researchers in <strong>large speech models</strong> and <strong>speech AI</strong>. 
                    Feel free to reach out if you'd like to discuss research collaborations or share insights!
                </p>
                <div style="display: flex; align-items: center; gap: 20px; flex-wrap: wrap;">
                    <div style="display: flex; align-items: center; gap: 8px;">
                        <i class="fa fa-envelope" style="color: #0085a1; font-size: 18px;"></i>
                        <a href="mailto:zhuxiaoxuhit@gmail.com" style="color: #0085a1; text-decoration: none; font-weight: 500;">
                            zhuxiaoxuhit@gmail.com
                        </a>
                    </div>
                    <div style="display: flex; align-items: center; gap: 8px;">
                        <i class="fa fa-wechat" style="color: #0085a1; font-size: 18px;"></i>
                        <button onclick="toggleWechatQR()" style="background: #0085a1; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; font-size: 14px;">
                            Show WeChat QR Code
                        </button>
                    </div>
                </div>
                <div id="wechat-qr" style="display: none; margin-top: 20px; text-align: center;">
                    <img src="/img/wechat.png" alt="WeChat QR Code" style="max-width: 200px; border: 1px solid #ddd; border-radius: 8px;">
                    <p style="color: #666; font-size: 14px; margin-top: 10px;">Scan to add me on WeChat</p>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- JavaScript for Publications -->
<script>
function toggleAbstract(id) {
    var element = document.getElementById(id);
    if (element.style.display === "none" || element.style.display === "") {
        element.style.display = "block";
        // Hide any open bibtex
        var bibtexId = id.replace('abstract', 'bibtex');
        var bibtexElement = document.getElementById(bibtexId);
        if (bibtexElement) {
            bibtexElement.style.display = "none";
        }
    } else {
        element.style.display = "none";
    }
}

function toggleBibtex(id) {
    var element = document.getElementById(id);
    if (element.style.display === "none" || element.style.display === "") {
        element.style.display = "block";
        // Hide any open abstract
        var abstractId = id.replace('bibtex', 'abstract');
        var abstractElement = document.getElementById(abstractId);
        if (abstractElement) {
            abstractElement.style.display = "none";
        }
    } else {
        element.style.display = "none";
    }
}

function toggleWechatQR() {
    var qrElement = document.getElementById('wechat-qr');
    var button = event.target;
    
    if (qrElement.style.display === "none" || qrElement.style.display === "") {
        qrElement.style.display = "block";
        button.textContent = "Hide WeChat QR Code";
        button.style.background = "#d9534f";
    } else {
        qrElement.style.display = "none";
        button.textContent = "Show WeChat QR Code";
        button.style.background = "#0085a1";
    }
}
</script>
