---
layout: default
title: "Home"
description: "Speech AI researcher focusing on large speech models and speech generation"
---

<!-- About Me Section -->
<div class="container" style="margin-top: 50px;">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="site-heading">
                <h1 style="margin-bottom: 30px;"><i class="fa fa-home"></i> About Me</h1>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 16px; line-height: 1.6;">
                    I am Xiaoxu Zhu (朱晓旭), a Speech AI researcher focusing on <strong>large speech models</strong> and <strong>speech generation</strong>. 
                    I have been working on speech algorithm research and development at <strong>SenseTime</strong> since 2021. I have also worked or interned at <strong>Siemens</strong>, <strong>Cheetah Mobile (OrionStar)</strong>, and <strong>Shanghai AI Laboratory</strong>.
                </p>
                <p style="font-size: 16px; line-height: 1.6;">
                    I received my Bachelor's degree in Materials Forming and Control Engineering from <strong>Harbin Institute of Technology</strong> in 2016, 
                    and my Master's degree in Informatics and Computing Technology from <strong>Peter the Great St. Petersburg Polytechnic University</strong> in 2019. 
                    I am currently pursuing my Master's degree in Information Management at <strong>Tsinghua University</strong>, expected to graduate in 2025.
                </p>
            </div>
            
            <!-- Publications Section -->
            <div id="publications" style="margin-top: 50px;">
                <h2 style="color: #333; margin-bottom: 30px;"><i class="fa fa-book"></i> Publications</h2>
                
                <!-- Publication 1 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_1.png" alt="Paper 1" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability</h4>
                            <p style="color: #666; margin: 5px 0;">
                                <strong>Xiaoxu Zhu</strong>, Junhua Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Technical Report</em>, 2024
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://arxiv.org/pdf/2507.17851" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract1')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex1')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    Speech pretrained models contain task-specific information across different layers, but decoupling content and timbre information remains challenging as removing speaker-specific information often causes content loss. Current research lacks direct metrics to quantify timbre residual in model encodings, relying on indirect evaluation through downstream tasks. This paper addresses these challenges through interpretability-based speaker disentanglement in speech pretraining models. We quantitatively evaluate timbre residual in model embeddings and improve speaker disentanglement using interpretive representations.
                                </p>
                            </div>
                            <div id="bibtex1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@article{zhu2024speaker,
  title={Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability},
  author={Zhu, Xiaoxu and Li, Junhua},
  journal={arXiv preprint arXiv:2507.17851},
  year={2024}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 2 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_2.png" alt="Paper 2" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese</h4>
                            <p style="color: #666; margin: 5px 0;">
                                Song Zhang, Ken Zheng, <strong>Xiaoxu Zhu</strong>, Baoxiang Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Interspeech</em>, 2022
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://www.isca-archive.org/interspeech_2022/zhang22b_interspeech.pdf" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract2')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex2')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. We create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation, then extend a pre-trained Chinese BERT with these new tokens and corresponding embedding layers initialized by the embeddings of source polyphonic characters.
                                </p>
                            </div>
                            <div id="bibtex2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{zhang2022polyphone,
  title={A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese},
  author={Zhang, Song and Zheng, Ken and Zhu, Xiaoxu and Li, Baoxiang},
  booktitle={Proc. Interspeech 2022},
  year={2022},
  note={Accepted for INTERSPEECH 2022}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 3 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_3.png" alt="Paper 3" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection</h4>
                            <p style="color: #666; margin: 5px 0;">
                                Yiwei Wei, Shaozu Yuan, Ruosong Yang, Lei Shen, Longbiao Wang, <strong>Xiaoxu Zhu</strong>, Meng Chen
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>ACL</em>, 2023
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://aclanthology.org/2023.acl-long.287.pdf" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract3')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex3')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    With the popularity of social media, detecting sentiment from multimodal posts has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. We propose a novel Multi-View Calibration Network (MVCN) to alleviate modality heterogeneity issues systematically. We first propose a text-guided fusion module with novel Sparse-Attention to reduce the negative impacts of redundant visual elements. We then devise a sentiment-based congruity constraint task to calibrate the feature shift in the representation space. Finally, we introduce an adaptive loss calibration strategy to tackle inconsistent annotated labels.
                                </p>
                            </div>
                            <div id="bibtex3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{wei2023tackling,
  title={Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection},
  author={Wei, Yiwei and Yuan, Shaozu and Yang, Ruosong and Shen, Lei and Wang, Longbiao and Zhu, Xiaoxu and Chen, Meng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5140--5156},
  year={2023}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- JavaScript for Publications -->
<script>
function toggleAbstract(id) {
    var element = document.getElementById(id);
    if (element.style.display === "none" || element.style.display === "") {
        element.style.display = "block";
        // Hide any open bibtex
        var bibtexId = id.replace('abstract', 'bibtex');
        var bibtexElement = document.getElementById(bibtexId);
        if (bibtexElement) {
            bibtexElement.style.display = "none";
        }
    } else {
        element.style.display = "none";
    }
}

function toggleBibtex(id) {
    var element = document.getElementById(id);
    if (element.style.display === "none" || element.style.display === "") {
        element.style.display = "block";
        // Hide any open abstract
        var abstractId = id.replace('bibtex', 'abstract');
        var abstractElement = document.getElementById(abstractId);
        if (abstractElement) {
            abstractElement.style.display = "none";
        }
    } else {
        element.style.display = "none";
    }
}
</script>
