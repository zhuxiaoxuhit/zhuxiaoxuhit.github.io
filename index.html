---
layout: default
title: "Home"
description: "Speech AI researcher focusing on large speech models and speech generation"
---

<!-- About Me Section -->
<div class="container" style="margin-top: 50px;">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="site-heading">
                <h1 style="margin-bottom: 30px;"><i class="fa fa-home"></i> About Me</h1>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 16px; line-height: 1.6;">
                    I am Xiaoxu Zhu (朱晓旭), a Speech AI researcher focusing on <strong>large speech models</strong> and <strong>speech generation</strong>. 
                    I have been working on speech algorithm research and development at <strong>SenseTime</strong> since 2021. I have also worked or interned at <strong>Siemens</strong>, <strong>Cheetah Mobile (OrionStar)</strong>, and <strong>Shanghai AI Laboratory</strong>.
                </p>
                <p style="font-size: 16px; line-height: 1.6;">
                    I received my Bachelor's degree in Materials Forming and Control Engineering from <strong>Harbin Institute of Technology</strong> in 2016, 
                    and my Master's degree in Informatics and Computing Technology from <strong>Peter the Great St. Petersburg Polytechnic University</strong> in 2019. 
                    I am currently pursuing my Master's degree in Information Management at <strong>Tsinghua University</strong>, expected to graduate in 2025.
                </p>
            </div>
            
            <!-- Publications Section -->
            <div id="publications" style="margin-top: 50px;">
                <h2 style="color: #333; margin-bottom: 30px;"><i class="fa fa-book"></i> Publications</h2>
                
                <!-- Publication 1 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_1.png" alt="Paper 1" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                            <p style="text-align: center; font-size: 12px; color: #666; margin-top: 5px;">Figure 1</p>
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability</h4>
                            <p style="color: #666; margin: 5px 0;">
                                <strong>Xiaoxu Zhu</strong>, Junhua Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Technical Report</em>, 2024
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://arxiv.org/pdf/2507.17851" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract1')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex1')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    This work focuses on speaker disentanglement in speech pre-trained models. We explore interpretability methods to understand how speaker information is encoded in large speech models and propose techniques to separate speaker-specific features from content features. Our approach enables better control over speaker characteristics in speech generation tasks while maintaining content quality.
                                </p>
                            </div>
                            <div id="bibtex1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@article{zhu2024speaker,
  title={Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability},
  author={Zhu, Xiaoxu and Li, Junhua},
  journal={Technical Report},
  year={2024}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 2 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_2.png" alt="Paper 2" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                            <p style="text-align: center; font-size: 12px; color: #666; margin-top: 5px;">Figure 2</p>
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese</h4>
                            <p style="color: #666; margin: 5px 0;">
                                Song Zhang, Ken Zheng, <strong>Xiaoxu Zhu</strong>, Baoxiang Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Interspeech</em>, 2022
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://www.isca-archive.org/interspeech_2022/zhang22b_interspeech.pdf" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract2')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex2')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    This paper presents a polyphone BERT model specifically designed for Mandarin Chinese polyphone disambiguation. We leverage the pre-trained BERT architecture and fine-tune it for the task of predicting correct pronunciations of polyphonic characters in Chinese text. The model achieves significant improvements in accuracy compared to traditional methods and shows robust performance across different domains.
                                </p>
                            </div>
                            <div id="bibtex2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{zhang2022polyphone,
  title={A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese},
  author={Zhang, Song and Zheng, Ken and Zhu, Xiaoxu and Li, Baoxiang},
  booktitle={Proc. Interspeech 2022},
  pages={1953--1957},
  year={2022}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 3 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_3.png" alt="Paper 3" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                            <p style="text-align: center; font-size: 12px; color: #666; margin-top: 5px;">Figure 3</p>
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy</h4>
                            <p style="color: #666; margin: 5px 0;">
                                C. Ding, D. Zong, B. Li, S. Zhang, <strong>X. Zhu</strong>, G. Zhong, D. Zhou
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>IEEE/ACM MuSe-Mimic</em>, 2023
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://dl.acm.org/doi/10.1145/3606039.3613113" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract3')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex3')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    This paper proposes an efficient multimodal transformer for sentiment analysis that integrates text, audio, and visual modalities. We introduce a modality-aware adaptive training strategy that dynamically adjusts the learning process based on the contribution of each modality. Our approach achieves state-of-the-art performance on the MuSe-Mimic challenge while maintaining computational efficiency.
                                </p>
                            </div>
                            <div id="bibtex3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{ding2023multimodal,
  title={Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy},
  author={Ding, C and Zong, D and Li, B and Zhang, S and Zhu, X and Zhong, G and Zhou, D},
  booktitle={Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop},
  pages={39--46},
  year={2023}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
