---
layout:     post
title:      [多音字消歧] Knowledge Distillation from Bert in Pre-Training and Fine-Tuning for Polyphone Disambiguation 论文解读 
subtitle:   A Mask-based Model for Mandarin Chinese Polyphone Disambiguation
date:       2021-04-12
author:     朱晓旭
header-img: img/bg.jpg
catalog: true
tags:
    - Front End
    - Polyphone Disambiguation
    - mask vector
    - BERT 
    - fine-tuning 
    - knowledge distillation 
---
![](/img/poly_bert_1.jpg)
![](/img/poly_bert_2.jpg)
![](/img/poly_bert_3.jpg)
![](/img/poly_bert_4.jpg)
![](/img/poly_bert_5.jpg)
![](/img/poly_bert_6.jpg)
![](/img/poly_bert_7.jpg)
![](/img/poly_bert_8.jpg)
![](/img/poly_bert_9.jpg)
![](/img/poly_bert_10.jpg)
![](/img/poly_bert_11.jpg)
![](/img/poly_bert_12.jpg)
![](/img/poly_bert_13.jpg)

