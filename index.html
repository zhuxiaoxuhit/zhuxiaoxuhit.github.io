---
layout: default
title: "Home"
description: "Speech AI researcher focusing on large speech models and speech generation"
---

<!-- About Me Section -->
<div class="container" style="margin-top: 50px;">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="site-heading">
                <h1 style="margin-bottom: 30px;"><i class="fa fa-home"></i> About Me</h1>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 16px; line-height: 1.6;">
                    I am Xiaoxu Zhu (朱晓旭), a Speech AI researcher focusing on <strong>large speech models</strong> and <strong>speech generation</strong>. 
                    I have been working on speech algorithm research and development at <strong>SenseTime</strong> since 2021. I have also worked or interned at <strong>Siemens</strong>, <strong>Cheetah Mobile (OrionStar)</strong>, and <strong>Shanghai AI Laboratory</strong>.
                </p>
                <p style="font-size: 16px; line-height: 1.6;">
                    I received my Bachelor's degree in Materials Forming and Control Engineering from <strong>Harbin Institute of Technology</strong> in 2016, 
                    and my Master's degree in Informatics and Computing Technology from <strong>Peter the Great St. Petersburg Polytechnic University</strong> in 2019. 
                    I am currently pursuing my Master's degree in Information Management at <strong>Tsinghua University</strong>, expected to graduate in 2025.
                </p>
            </div>
            
            <!-- Publications Section -->
            <div id="publications" style="margin-top: 50px;">
                <h2 style="color: #333; margin-bottom: 30px;"><i class="fa fa-book"></i> Publications</h2>
                
                <!-- Publication 1 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_1.png" alt="Paper 1" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability</h4>
                            <p style="color: #666; margin: 5px 0;">
                                <strong>Xiaoxu Zhu</strong>, Junhua Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Technical Report</em>, 2024
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://arxiv.org/pdf/2507.17851" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract1')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex1')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    Speech pretrained models contain task-specific information across different layers, but decoupling content and timbre information remains challenging as removing speaker-specific information often causes content loss. Current research lacks direct metrics to quantify timbre residual in model encodings, relying on indirect evaluation through downstream tasks. This paper addresses these challenges through interpretability-based speaker disentanglement in speech pretraining models. We quantitatively evaluate timbre residual in model embeddings and improve speaker disentanglement using interpretive representations.
                                </p>
                            </div>
                            <div id="bibtex1" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@article{zhu2024speaker,
  title={Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability},
  author={Zhu, Xiaoxu and Li, Junhua},
  journal={arXiv preprint arXiv:2507.17851},
  year={2024}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 2 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_2.png" alt="Paper 2" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese</h4>
                            <p style="color: #666; margin: 5px 0;">
                                Song Zhang, Ken Zheng, <strong>Xiaoxu Zhu</strong>, Baoxiang Li
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>Interspeech</em>, 2022
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://www.isca-archive.org/interspeech_2022/zhang22b_interspeech.pdf" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract2')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex2')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. We create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation, then extend a pre-trained Chinese BERT with these new tokens and corresponding embedding layers initialized by the embeddings of source polyphonic characters.
                                </p>
                            </div>
                            <div id="bibtex2" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{zhang2022polyphone,
  title={A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese},
  author={Zhang, Song and Zheng, Ken and Zhu, Xiaoxu and Li, Baoxiang},
  booktitle={Proc. Interspeech 2022},
  year={2022},
  note={Accepted for INTERSPEECH 2022}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Publication 3 -->
                <div class="publication-item" style="margin-bottom: 40px; padding: 10px 0;">
                    <div class="row">
                        <div class="col-md-3">
                            <img src="/img/paper_placeholder_3.png" alt="Paper 3" class="img-responsive" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                        </div>
                        <div class="col-md-9">
                            <h4 style="margin-top: 0; color: #333;">Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy</h4>
                            <p style="color: #666; margin: 5px 0;">
                                C. Ding, D. Zong, B. Li, S. Zhang, <strong>X. Zhu</strong>, G. Zhong, D. Zhou
                            </p>
                            <p style="color: #0085a1; margin: 5px 0;">
                                <em>IEEE/ACM MuSe-Mimic</em>, 2023
                            </p>
                            <div style="margin-top: 10px;">
                                <a href="https://dl.acm.org/doi/10.1145/3606039.3613113" target="_blank" style="background: #0085a1; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">paper</a>
                                <a href="javascript:void(0)" onclick="toggleAbstract('abstract3')" style="background: #5cb85c; color: white; margin-right: 10px; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">abstract</a>
                                <a href="javascript:void(0)" onclick="toggleBibtex('bibtex3')" style="background: #f0ad4e; color: white; text-decoration: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;">bibtex</a>
                            </div>
                            <div id="abstract3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #5cb85c; border-radius: 5px;">
                                <p style="color: #333; font-size: 14px; line-height: 1.6;">
                                    This paper proposes an efficient multimodal transformer for sentiment analysis that integrates text, audio, and visual modalities. We introduce a modality-aware adaptive training strategy that dynamically adjusts the learning process based on the contribution of each modality. Our approach achieves state-of-the-art performance on the MuSe-Mimic challenge while maintaining computational efficiency.
                                </p>
                            </div>
                            <div id="bibtex3" style="display: none; margin-top: 15px; padding: 15px; background: #f8f9fa; border-left: 4px solid #f0ad4e; border-radius: 5px;">
                                <pre style="color: #333; font-size: 12px; margin: 0; white-space: pre-wrap;">@inproceedings{ding2023multimodal,
  title={Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy},
  author={Ding, C and Zong, D and Li, B and Zhang, S and Zhu, X and Zhong, G and Zhou, D},
  booktitle={Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop},
  pages={39--46},
  year={2023}
}</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- JavaScript for Publications -->
<script>
function toggleAbstract(id) {
    var element = document.getElementById(id);
    if (element.style.display === "none" || element.style.display === "") {
        element.style.display = "block";
        // Hide any open bibtex
        var bibtexId = id.replace('abstract', 'bibtex');
        var bibtexElement = document.getElementById(bibtexId);
        if (bibtexElement) {
            bibtexElement.style.display = "none";
        }
    } else {
        element.style.display = "none";
    }
}

function toggleBibtex(id) {
    var element = document.getElementById(id);
    if (element.style.display === "none" || element.style.display === "") {
        element.style.display = "block";
        // Hide any open abstract
        var abstractId = id.replace('bibtex', 'abstract');
        var abstractElement = document.getElementById(abstractId);
        if (abstractElement) {
            abstractElement.style.display = "none";
        }
    } else {
        element.style.display = "none";
    }
}
</script>
