---
layout:     post
title:      支持向量机
subtitle:   Support Vector Machine
date:       2020-03-10
author:     朱晓旭
header-img: img/bg.jpg
catalog: true
tags:
    - 支持向量机
    - SVM
    - Support Vector Machine
    - 最优化
    - 核方法
    - kernel method
    - 对偶	
    - 拉格朗日乘子法	
    - KKT条件	
---


# 支持向量机
# 前言
支持向量机(support vector machine)是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。
以上是维基百科的对SVM的定义。      
用通俗的话讲，它是一种统计机器学习的分类方法，根据训练集分布去寻找一个最大间隔超平面。1963年被提出，92年通过核方法可以实现非线性分类。
支持向量机（SVM）算法在分类问题中有着重要地位，其主要思想是最大化两类之间的间隔。按照数据集的特点：

1.  线性可分问题，如之前的感知机算法处理的问题
2.  线性可分，只有一点点错误点，如感知机算法发展出来的 Pocket 算法处理的问题
3.  非线性问题，完全不可分，如在感知机问题发展出来的多层感知机和深度学习

这三种情况对于 SVM 分别有下面三种处理手段：

1.  hard-margin SVM
2.  soft-margin SVM
3.  kernel Method

# hard-margin SVM
数据是data:
<center>$$\{(x_{i},y_{i})\}_{i=1}^{N}$$</center>
<center>$$y_{i}\in\{1,-1\},x_{i}\in{\mathbb{R}}^{p}$$</center>
分类器的预测目标是：给定x情况下，预测y属于哪类。   
假设数据是线性可粉的，超平面的约束条件：   
<center>$$$$</center>
<center>$$w^{T}x_{i}+b>0,y_{i}=1$$</center>
<center>$$w^{T}x_{i}+b<0,y_{i}=-1$$</center>
即：  
<center>$$y_{i}(w^{T}x_{i}+b)>0,for i in 1,2,3...,N $$</center>
这样的超平面存在多个   
![](/img/svm_1.jpg)
很自然的，最优的超平面是最大间隔超平面：     
间隔是指距离超平面最近的点与超平面的距离。    
<center>$$margin(w,b) = \min_{w,b,x_{i},i in 1,2,3..N}distance(w,b,x_{i}) = \min_{w,b,x_{i},i in 1,2,3..N}\frac{1}{\left\|w\right\|_2}|w^{T}x_{i}+b|$$</center>
我们需要寻找最大间隔的这样的一个超平面。     
![](svm_2.png)

由此，分类问题转化为了约束优化问题:     
<center>$$ \max_{w,b}\min_{x_{i},i in 1,2,3..N}\frac{1}{\left\|w\right\|_2}y_{i}(w^{T}x_{i}+b)=\max_{w,b}\frac{1}{\left\|w\right\|_2}\min_{x_{i},i in 1,2,3..N}y_{i}(w^{T}x_{i}+b)$$</center>
<center>$$s.t. y_{i}(w^{T}x_{i}+b)>0$$</center>
约束条件等价于：   
<center>$$\exists r>0,s.t. \min_{x_{i},y_{i},i in 1,2,3,4...N}y_{i}(w^{T}x_{i}+b)=r $$</center>
该r只要存在，我们可以对其任意放缩，表现在超平面上是系数乘以响应的倍数。为了方便计算，我们定为1。    
这样的话：    
<center>$$\max_{w,b}\frac{1}{\left\|w\right\|_2}$$</center>
<center>$$s.t. \min y_{i}(w^{T}x_{i}+b)=1 $$</center>
我们在求解最优化问题时，往往采用最小化：
<center>$$\min_{w,b}\frac{1}{2}w^{T}w$$</center>
<center>$$s.t.  \min y_{i}(w^{T}x_{i}+b)=1 $$</center>
应用[拉格朗日乘子法](https://zh.wikipedia.org/zh-cn/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)
<center>$$ L(w,b,a)=\frac{1}{2}w^{T}w+\sum_{i=1}^{N}a_{i}(1-y_{i}(w^{T}x_{i}+b))$$</center>
该式子来源于[南瓜书](https://datawhalechina.github.io/pumpkin-book/#/chapter6/chapter6)。
因此：   
<center>$$\min_{w,b}\max_{a}L(w,b,a)$$</center>
<center>$$s.t. a>=0 $$</center>
需要解释为什么a>=0:    
如果$(1-y_{i}(w^{T}x_{i}+b))>0,a可以是大于0的任意一个数，那$\max_{a}L(w,b,a)$本身是没有意义的。     
如果$(1-y_{i}(w^{T}x_{i}+b))<=0,a取0，和原式恒等。    
把原问题转换为求对偶问题：
<center>$$\max_{a}\min_{w,b}L(w,b,a)$$</center>
<center>$$s.t. a_{i}>=0 $$</center>
为什么把原问题转换为对偶问题：抛弃约束给a，因此可以可以通过求偏导求解$\min_{w,b}L(w,b,a)$,此使把a看作常量。			       
为什么是强对偶关系：二次凸优化问题本身满足KKT条件。本质上不理解。    
对w,b求偏导后：对b求偏导可以直接得出$\sum_{i=1}^{N}a_{i}y_{i}=0$
<center>$$\max_{a}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}x_{i}^{T}x_{j}-\sum_{i=1}^{N}a_{i}$$</center>
<center>$$s.t. a_{i}>=0 , \sum_{i=1}^{N}a_{i}y_{i}=0 $$</center>  
然后转为最小化问题求解a。

# soft-margin SVM
软间隔同样是针对线性可分问题。它的核心思想是：允许少许错误。
<center>$$\min_{w,b}\frac{1}{2}w^{T}w + loss$$</center>
loss可以是分错的数量，使用0/1损失函数。当数据存在噪声的时候用软间隔。

# kernel Method
当分类问题为线性不可分的下，使用核方法。    
当问题为线性不可分时，需要进行高维转换。因为低维不可分的数据，给与合适的维度转换，在高维下可实现分类。   
维度转换：  
<center>$$x_{i} -> f(x_{i})$$</center>
由此带来的问题时对偶问题带来的内积$x_{i},x_[j}^{T}$：
<center>$$<f(x_{i}),f(x_{i})^{T}>$$</center>
内积的计算复杂在很长一段时间无法实践，直到1992年核方法的到来。        
核方法思想：通俗的发讲，就是寻找高维维度转换f(x),使我们能够轻松计算内积。核函数就是他们的内积表示。       
我们使用的时候一般通过差表选择合适的核函数。   
![](/img/svm_3.jpg)


### 参考

- [机器学习-周志华(西瓜书)](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)





